{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/febrahim-driod/hello-world/blob/master/Yolov8_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) and check software and hardware."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG"
      },
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "metadata": {
        "id": "DTjhFMyhdWSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f335008a-15fd-4d47-e4b7-3a17061be1f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow --quiet\n",
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"P5E6JhfkeiA1nAdZAWnB\")\n",
        "project = rf.workspace(\"sinfo\").project(\"car_damage-4xqh8\")\n",
        "dataset = project.version(6).download(\"yolov8\")\n"
      ],
      "metadata": {
        "id": "-6KSPpfndo2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clearml --quiet\n",
        "\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "# colob_notebook\n",
        "%env CLEARML_API_ACCESS_KEY=AMY0O903Z5A7KN5FETFJ\n",
        "%env CLEARML_API_SECRET_KEY=y7hUwY62xMFWwADGFWzKd33C2b8C4f1AxTDymedak5lQFzmbJg\n"
      ],
      "metadata": {
        "id": "qDxE57EKe8VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9d6cae-480c-4569-fedc-366e72b0aa18"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=AMY0O903Z5A7KN5FETFJ\n",
            "env: CLEARML_API_SECRET_KEY=y7hUwY62xMFWwADGFWzKd33C2b8C4f1AxTDymedak5lQFzmbJg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_NAME=\"yolov8_damage_detection\"\n",
        "RUN_NAME=\"train-seg\"\n",
        "\n",
        "from clearml import Task\n",
        "\n",
        "task = Task.init(project_name=PROJECT_NAME, task_name=RUN_NAME)"
      ],
      "metadata": {
        "id": "65bNwW2EnBya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aac2d91-7000-43c4-9d98-fc23749d39ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClearML Task: created new task id=ef23530aad504d8f98d6e315547a62c3\n",
            "ClearML results page: https://app.clear.ml/projects/ae6ddbd949a949299cd90e56ad876556/experiments/ef23530aad504d8f98d6e315547a62c3/output/log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Predict\n",
        "\n",
        "YOLOv8 may be used directly in the Command Line Interface (CLI) with a `yolo` command for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See a full list of available `yolo` [arguments](https://docs.ultralytics.com/usage/cfg/) in the YOLOv8 [Docs](https://docs.ultralytics.com).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR9ZbuQCH7FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523c3c99-66ae-4635-b217-447c4b4a7292"
      },
      "source": [
        "# Run inference on an image with YOLOv8n-seg\n",
        "!yolo mode='predict' \\\n",
        "      task='segment' \\\n",
        "      model=yolov8x-seg.pt \\\n",
        "      source='https://ultralytics.com/images/zidane.jpg' \\\n",
        "      box=False \\\n",
        "      visualize=True "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-seg.pt to yolov8x-seg.pt...\n",
            "100% 137M/137M [00:02<00:00, 64.6MB/s]\n",
            "Ultralytics YOLOv8.0.55 ðŸš€ Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8x-seg summary (fused): 295 layers, 71797696 parameters, 0 gradients, 344.1 GFLOPs\n",
            "\n",
            "Found https://ultralytics.com/images/zidane.jpg locally at zidane.jpg\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "visualize feature not yet supported\n",
            "image 1/1 /content/zidane.jpg: 384x640 3 persons, 2 ties, 56.1ms\n",
            "Speed: 0.7ms preprocess, 56.1ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WEIGHTS_PATH = f\"{HOME}/yolov8x-seg.pt\"\n",
        "DATA_YAML_PATH= f\"{HOME}/car_damage-6/data.yaml\"\n",
        "TASK='segment'\n",
        "IMGSZ=640\n",
        "BATCH_SIZE=16\n",
        "WORKERS=8\n",
        "EPOCH=50\n",
        "PATIENCE=5"
      ],
      "metadata": {
        "id": "YYADU31VfS9N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"1000\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\"/></a></p>\n",
        "\n",
        "Train YOLOv8 on [Detection](https://docs.ultralytics.com/tasks/detect/), [Segmentation](https://docs.ultralytics.com/tasks/segment/) and [Classification](https://docs.ultralytics.com/tasks/classify/) datasets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_YAML_PATH"
      ],
      "metadata": {
        "id": "YQLvqmuJgIkd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e8ccb2e-b011-4980-c960-44c10084ecc2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/car_damage-6/data.yaml'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217859ae-2075-4c3f-9e33-0f95c6e62931"
      },
      "source": [
        "# Train YOLOv8m on Custom Data\n",
        "!yolo mode=train \\\n",
        "      model=$WEIGHTS_PATH \\\n",
        "      data=$DATA_YAML_PATH \\\n",
        "      epochs=$EPOCH \\\n",
        "      imgsz=$IMGSZ \\\n",
        "      task=$TASK \\\n",
        "      project=$PROJECT_NAME \\\n",
        "      name=$RUN_NAME \\\n",
        "      batch=$BATCH_SIZE \\\n",
        "      workers=$WORKERS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.55 ðŸš€ Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=/content/yolov8x-seg.pt, data=/content/car_damage-6/data.yaml, epochs=50, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=yolov8_damage_detection, name=train-seg, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=yolov8_damage_detection/train-seg\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2320  ultralytics.nn.modules.Conv                  [3, 80, 3, 2]                 \n",
            "  1                  -1  1    115520  ultralytics.nn.modules.Conv                  [80, 160, 3, 2]               \n",
            "  2                  -1  3    436800  ultralytics.nn.modules.C2f                   [160, 160, 3, True]           \n",
            "  3                  -1  1    461440  ultralytics.nn.modules.Conv                  [160, 320, 3, 2]              \n",
            "  4                  -1  6   3281920  ultralytics.nn.modules.C2f                   [320, 320, 6, True]           \n",
            "  5                  -1  1   1844480  ultralytics.nn.modules.Conv                  [320, 640, 3, 2]              \n",
            "  6                  -1  6  13117440  ultralytics.nn.modules.C2f                   [640, 640, 6, True]           \n",
            "  7                  -1  1   3687680  ultralytics.nn.modules.Conv                  [640, 640, 3, 2]              \n",
            "  8                  -1  3   6969600  ultralytics.nn.modules.C2f                   [640, 640, 3, True]           \n",
            "  9                  -1  1   1025920  ultralytics.nn.modules.SPPF                  [640, 640, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 12                  -1  3   7379200  ultralytics.nn.modules.C2f                   [1280, 640, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 15                  -1  3   1948800  ultralytics.nn.modules.C2f                   [960, 320, 3]                 \n",
            " 16                  -1  1    922240  ultralytics.nn.modules.Conv                  [320, 320, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 18                  -1  3   7174400  ultralytics.nn.modules.C2f                   [960, 640, 3]                 \n",
            " 19                  -1  1   3687680  ultralytics.nn.modules.Conv                  [640, 640, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 21                  -1  3   7379200  ultralytics.nn.modules.C2f                   [1280, 640, 3]                \n",
            " 22        [15, 18, 21]  1  12318134  ultralytics.nn.modules.Segment               [2, 32, 320, [320, 640, 640]] \n",
            "YOLOv8x-seg summary: 401 layers, 71752774 parameters, 71752758 gradients, 344.5 GFLOPs\n",
            "\n",
            "Transferred 651/657 items from pretrained weights\n",
            "2023-03-22 21:45:26.747906: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-22 21:45:27.832201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-22 21:45:27.832354: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-22 21:45:27.832383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov8_damage_detection/train-seg', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 106 weight(decay=0.0), 117 weight(decay=0.0005), 116 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/car_damage-6/train/labels.cache... 3016 images, 0 backgrounds, 0 corrupt: 100% 3016/3016 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/car_damage-6/valid/labels.cache... 862 images, 0 backgrounds, 0 corrupt: 100% 862/862 [00:00<?, ?it/s]\n",
            "Plotting labels to yolov8_damage_detection/train-seg/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1myolov8_damage_detection/train-seg\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/50      14.3G      1.849      3.962      3.003      2.058         16        640: 100% 189/189 [05:17<00:00,  1.68s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:39<00:00,  1.45s/it]\n",
            "                   all        862       1658      0.254      0.238      0.152     0.0661      0.224      0.214      0.118     0.0427\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/50      14.2G       1.73      3.385      2.306      1.852         30        640: 100% 189/189 [05:17<00:00,  1.68s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.42s/it]\n",
            "                   all        862       1658      0.355      0.258      0.224     0.0907       0.32      0.218      0.173     0.0523\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/50      14.2G      1.813      3.485      2.336       1.93         30        640: 100% 189/189 [05:14<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.42s/it]\n",
            "                   all        862       1658      0.216      0.257      0.124      0.045      0.181      0.186     0.0782     0.0223\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/50      14.2G      1.915      3.614      2.435      2.033         20        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.42s/it]\n",
            "                   all        862       1658      0.275      0.227      0.169     0.0614      0.233       0.18      0.115     0.0346\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/50      14.2G      1.896      3.612      2.418      2.035         26        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.41s/it]\n",
            "                   all        862       1658      0.269      0.256       0.17     0.0632      0.247      0.218       0.13     0.0401\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/50      14.2G      1.876      3.544      2.414      2.011         20        640: 100% 189/189 [05:14<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.44s/it]\n",
            "                   all        862       1658      0.341      0.269      0.231     0.0893      0.305      0.233       0.18     0.0584\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/50      14.2G      1.838      3.499      2.334       1.99         40        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:39<00:00,  1.46s/it]\n",
            "                   all        862       1658      0.304      0.255      0.205     0.0808      0.285      0.227      0.171     0.0549\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/50      14.2G      1.795      3.421      2.272      1.948         22        640: 100% 189/189 [05:15<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:39<00:00,  1.45s/it]\n",
            "                   all        862       1658      0.369      0.308      0.251      0.102      0.361       0.28      0.219     0.0717\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/50      14.2G       1.77      3.322      2.188      1.908         23        640: 100% 189/189 [05:14<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.43s/it]\n",
            "                   all        862       1658      0.378      0.285      0.253      0.104      0.328      0.259      0.193     0.0643\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/50      14.2G      1.752      3.312      2.164      1.902         33        640: 100% 189/189 [05:14<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.43s/it]\n",
            "                   all        862       1658      0.392      0.328      0.296      0.123      0.358      0.294      0.243     0.0798\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/50      14.2G      1.709      3.255      2.104      1.864         23        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.43s/it]\n",
            "                   all        862       1658      0.431      0.349      0.305      0.136      0.379      0.313      0.253     0.0875\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/50      14.2G      1.675      3.201      2.031      1.845         28        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.43s/it]\n",
            "                   all        862       1658      0.469      0.328      0.341      0.147      0.429       0.29      0.272     0.0941\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/50      14.2G      1.668      3.154      2.023      1.821         31        640: 100% 189/189 [05:15<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:39<00:00,  1.46s/it]\n",
            "                   all        862       1658      0.442      0.375      0.339      0.151      0.403      0.342      0.291      0.105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/50      14.2G      1.633      3.086      1.929      1.788         23        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.44s/it]\n",
            "                   all        862       1658      0.428      0.362      0.343      0.159      0.413      0.329      0.299      0.114\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/50      14.2G      1.608      3.025       1.87      1.768         43        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.44s/it]\n",
            "                   all        862       1658      0.455      0.366      0.354      0.166       0.45      0.333      0.316      0.119\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/50      14.2G      1.564      3.016      1.816      1.746         34        640: 100% 189/189 [05:14<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.42s/it]\n",
            "                   all        862       1658      0.504      0.373      0.381      0.182      0.467      0.349       0.33      0.122\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/50      14.2G      1.582      2.987      1.797      1.751         26        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.42s/it]\n",
            "                   all        862       1658       0.46      0.363       0.34       0.16      0.447      0.353      0.317      0.115\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/50      14.2G      1.548      2.945      1.766      1.715         19        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:39<00:00,  1.45s/it]\n",
            "                   all        862       1658      0.473        0.4      0.388       0.19      0.464      0.353      0.336      0.133\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/50      14.2G      1.526      2.883      1.705      1.699         23        640: 100% 189/189 [05:15<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.42s/it]\n",
            "                   all        862       1658      0.533        0.4      0.413      0.203      0.503      0.372      0.371      0.142\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/50      14.2G      1.501      2.852      1.685      1.682         33        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.44s/it]\n",
            "                   all        862       1658      0.544        0.4      0.419      0.201      0.526      0.381      0.386      0.148\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/50      14.2G      1.479      2.819      1.623      1.665         46        640: 100% 189/189 [05:14<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.42s/it]\n",
            "                   all        862       1658      0.491      0.413      0.419      0.202      0.444      0.373       0.35      0.135\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/50      14.2G      1.475      2.785      1.588      1.648         29        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.43s/it]\n",
            "                   all        862       1658      0.502      0.416      0.409      0.201      0.491      0.381      0.371      0.146\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/50      14.2G      1.432      2.735      1.543      1.616         32        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.43s/it]\n",
            "                   all        862       1658      0.568      0.402      0.442      0.219      0.511      0.401        0.4      0.161\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/50      14.2G      1.407      2.683      1.502      1.598         31        640: 100% 189/189 [05:14<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.41s/it]\n",
            "                   all        862       1658      0.534      0.454      0.461      0.233      0.508      0.419      0.419      0.171\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/50      14.2G      1.383      2.627      1.438      1.577         28        640: 100% 189/189 [05:15<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.42s/it]\n",
            "                   all        862       1658      0.553      0.419      0.437      0.219      0.549      0.388      0.402      0.165\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      26/50      14.2G      1.374      2.597        1.4      1.559         22        640: 100% 189/189 [05:15<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.43s/it]\n",
            "                   all        862       1658      0.546      0.449      0.471      0.245      0.556      0.429      0.446      0.183\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      27/50      14.2G      1.353      2.582      1.376      1.557         27        640: 100% 189/189 [05:14<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.44s/it]\n",
            "                   all        862       1658       0.54       0.45      0.455      0.232      0.534      0.409      0.412      0.169\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      28/50      14.2G      1.336      2.516      1.336      1.531         21        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.43s/it]\n",
            "                   all        862       1658      0.536      0.466      0.462      0.236      0.499      0.437       0.41      0.167\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      29/50      14.2G      1.294      2.475      1.276       1.51         26        640: 100% 189/189 [05:12<00:00,  1.65s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.44s/it]\n",
            "                   all        862       1658      0.613      0.475      0.508      0.263      0.614      0.446       0.47      0.192\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      30/50      14.2G      1.265      2.402      1.221      1.475         16        640: 100% 189/189 [05:13<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.42s/it]\n",
            "                   all        862       1658      0.573      0.469      0.496      0.265      0.612       0.42      0.463      0.201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      31/50      14.2G      1.261      2.409      1.195      1.481         20        640: 100% 189/189 [05:15<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.43s/it]\n",
            "                   all        862       1658      0.605      0.458      0.497      0.264      0.591      0.446      0.458      0.193\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      32/50      14.2G      1.256      2.386       1.19      1.467         20        640: 100% 189/189 [05:14<00:00,  1.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:38<00:00,  1.42s/it]\n",
            "                   all        862       1658       0.58      0.468      0.504       0.27       0.59      0.443      0.472      0.203\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      33/50      14.2G      1.228      2.331      1.149      1.449         26        640: 100% 189/189 [05:14<00:00,  1.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% 27/27 [00:39<00:00,  1.45s/it]\n",
            "                   all        862       1658      0.595      0.483      0.513      0.274      0.573      0.468       0.47        0.2\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      34/50      14.2G      1.201      2.277      1.108      1.421         47        640:  80% 151/189 [04:13<01:02,  1.65s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import glob\n",
        "\n",
        "for imageName in glob.glob('/content/PROJECT_NAME/RUN_NAME/train_*.jpg'):\n",
        "      display(Image(filename=imageName))\n",
        "      print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QPLJEpqXppWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import glob\n",
        "\n",
        "for imageName in glob.glob('/content/PROJECT_NAME/RUN_NAME/val_*.jpg'):\n",
        "      display(Image(filename=imageName))\n",
        "      print(\"\\n\")"
      ],
      "metadata": {
        "id": "4q-4bZ6LrQX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Predict\n",
        "Predict a model's accuracy on the dataset's `test` splits. "
      ],
      "metadata": {
        "id": "VF17iJXgtChp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=$TASK \\\n",
        "      mode=predict \\\n",
        "      model='/content/PROJECT_NAME/RUN_NAME/weights/best.pt' \\\n",
        "      source='/content/car-damage-segmentation-2/test/images' \\\n",
        "      show=False \\\n",
        "      imgsz=$IMGSZ \\\n",
        "      project=$PROJECT_NAME \\\n",
        "      name=$RUN_NAME \\\n",
        "      hide_labels=True \\\n",
        "      conf=0.25 \\\n",
        "      save=True \\\n",
        "      box=False"
      ],
      "metadata": {
        "id": "4XDCTSF8tdLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import glob\n",
        "\n",
        "for imageName in glob.glob('/content/yolov8_damage_detection/train2/*.jpg')[90:100]:\n",
        "      display(Image(filename=imageName))\n",
        "      print(\"\\n\")"
      ],
      "metadata": {
        "id": "5Q33ZXT-t9A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Close clearML experiment\n",
        "task.close()"
      ],
      "metadata": {
        "id": "R0bZo4JJ5emI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Export\n",
        "\n",
        "Export a YOLOv8 model to any supported format with the `format` argument, i.e. `format=onnx`.\n",
        "\n",
        "- ðŸ’¡ ProTip: Export to [ONNX](https://onnx.ai/) or [OpenVINO](https://docs.openvino.ai/latest/index.html) for up to 3x CPU speedup.  \n",
        "- ðŸ’¡ ProTip: Export to [TensorRT](https://developer.nvidia.com/tensorrt) for up to 5x GPU speedup.\n",
        "\n",
        "\n",
        "| Format                                                                     | `format=`          | Model                     |\n",
        "|----------------------------------------------------------------------------|--------------------|---------------------------|\n",
        "| [PyTorch](https://pytorch.org/)                                            | -                  | `yolov8n.pt`              |\n",
        "| [TorchScript](https://pytorch.org/docs/stable/jit.html)                    | `torchscript`      | `yolov8n.torchscript`     |\n",
        "| [ONNX](https://onnx.ai/)                                                   | `onnx`             | `yolov8n.onnx`            |\n",
        "| [OpenVINO](https://docs.openvino.ai/latest/index.html)                     | `openvino`         | `yolov8n_openvino_model/` |\n",
        "| [TensorRT](https://developer.nvidia.com/tensorrt)                          | `engine`           | `yolov8n.engine`          |\n",
        "| [CoreML](https://github.com/apple/coremltools)                             | `coreml`           | `yolov8n.mlmodel`         |\n",
        "| [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`      | `yolov8n_saved_model/`    |\n",
        "| [TensorFlow GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`               | `yolov8n.pb`              |\n",
        "| [TensorFlow Lite](https://www.tensorflow.org/lite)                         | `tflite`           | `yolov8n.tflite`          |\n",
        "| [TensorFlow Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`          | `yolov8n_edgetpu.tflite`  |\n",
        "| [TensorFlow.js](https://www.tensorflow.org/js)                             | `tfjs`             | `yolov8n_web_model/`      |\n",
        "| [PaddlePaddle](https://github.com/PaddlePaddle)                            | `paddle`           | `yolov8n_paddle_model/`   |\n",
        "\n"
      ],
      "metadata": {
        "id": "nPZZeNrLCQG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model='/content/PROJECT_NAME/RUN_NAME/weights/best.pt' format=onnx"
      ],
      "metadata": {
        "id": "CYIjW4igCjqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Python Usage\n",
        "\n",
        "YOLOv8 was reimagined using Python-first principles for the most seamless Python YOLO experience yet. YOLOv8 models can be loaded from a trained checkpoint or created from scratch. Then methods are used to train, val, predict, and export the model. See a detailed Python usage examples in the YOLOv8 [Docs](https://docs.ultralytics.com/usage/python/)."
      ],
      "metadata": {
        "id": "kUMOQ0OeDBJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('yolov8n.yaml')  # build a new model from scratch\n",
        "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "results = model.train(data='coco128.yaml', epochs=3)  # train the model\n",
        "results = model.val()  # evaluate model performance on the validation set\n",
        "results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n",
        "success = model.export(format='onnx')  # export the model to ONNX format"
      ],
      "metadata": {
        "id": "bpF9-vS_DAaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Tasks\n",
        "\n",
        "YOLOv8 can train, val, predict and export models for the 3 primary tasks in vision AI: detection, segmentation and classification.\n",
        "\n",
        "<img width=\"1024\" src=\"https://user-images.githubusercontent.com/26833433/212094133-6bb8c21c-3d47-41df-a512-81c5931054ae.png\">\n"
      ],
      "metadata": {
        "id": "Phm9ccmOKye5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Detection\n",
        "\n",
        "YOLOv8 _detection_ models have no suffix and are the default YOLOv8 models, i.e. `yolov8n.pt` and are pretrained on COCO. See [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for full details.\n"
      ],
      "metadata": {
        "id": "yq26lwpYK1lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8n, train it on COCO128 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')  # load a pretrained YOLOv8n detection model\n",
        "model.train(data='coco128.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "8Go5qqS9LbC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Segmentation\n",
        "\n",
        "YOLOv8 _segmentation_ models use the `-seg` suffix, i.e. `yolov8n-seg.pt` and are pretrained on COCO. See [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for full details.\n"
      ],
      "metadata": {
        "id": "7ZW58jUzK66B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8n-seg, train it on COCO128-seg for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-seg.pt')  # load a pretrained YOLOv8n segmentation model\n",
        "model.train(data='coco128-seg.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "WFPJIQl_L5HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Classification\n",
        "\n",
        "YOLOv8 _classification_ models use the `-cls` suffix, i.e. `yolov8n-cls.pt` and are pretrained on ImageNet. See [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for full details.\n"
      ],
      "metadata": {
        "id": "ax3p94VNK9zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8n-cls, train it on mnist160 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-cls.pt')  # load a pretrained YOLOv8n classification model\n",
        "model.train(data='mnist160', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "5q9Zu6zlL5rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Additional content below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Git clone install (for development)\n",
        "!git clone https://github.com/ultralytics/ultralytics -b main\n",
        "%pip install -qe ultralytics"
      ],
      "metadata": {
        "id": "uRKlwxSJdhd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMusP4OAxFu6"
      },
      "source": [
        "# Run YOLOv8 tests (git clone install only)\n",
        "!pytest ultralytics/tests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate multiple models\n",
        "for x in 'nsmlx':\n",
        "  !yolo val model=yolov8{x}.pt data=coco.yaml"
      ],
      "metadata": {
        "id": "Wdc6t_bfzDDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}